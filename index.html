<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>Video Conference with Hand Tracking</title>
    <!-- Load MediaPipe Hands library for hand tracking -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
    <!-- Load MediaPipe Camera Utils for camera input handling -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
    <!-- Load MediaPipe Drawing Utils for rendering hand landmarks -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
    <!-- Load Socket.IO client for real-time communication -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/socket.io/4.7.5/socket.io.js"></script>
    <style>
             /* CSS to ensure full viewport coverage and remove margins/padding */
        body, html {
            margin: 0;
            padding: 0;
            height: 100%;
            width: 100%;
            overflow: hidden;
            display: flex;
            flex-direction: column;
            justify-content: center;
            align-items: center;
            background-color: #f0f0f0;
        }
                /* Container for both local and remote video elements */
        #video-container {
            display: flex;
            width: 100%;
            height: 80%;
            background-color: #ffffff;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
           /* Styling for each video wrapper */
        .video-wrapper {
            width: 50%;
            height: 100%;
            display: flex;
            justify-content: center;
            align-items: center;
            background-color: #f0f0f0;
            position: relative;
            border: 2px solid transparent; /* Initial border style */
        }
        .video-wrapper.active {
            border-color: lime; /* Active speaker highlight color */
        }
         /* Pseudo-element for displaying video labels */
        .video-wrapper::before {
            content: "";
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            font-size: 24px;
            color: #888888;
        }
          /* Styling for video elements */
        video {
            width: 100%;
            height: 100%;
            object-fit: contain;
            position: absolute;
        }
         /* Mirror the local video */
        #localVideo {
            transform: scaleX(-1);
        }
          /* Labels for local and remote video */
        #localVideo-wrapper::before {
            content: "Local Video";
        }
        #remoteVideo-wrapper::before {
            content: "Remote Video";
        }
             /* Styling for the canvas overlay (hand tracking visualization) */
        #outputCanvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            z-index: 0;
            pointer-events: none;
            transform: scaleX(-1);
        }
        #controls {
            display: flex;
            justify-content: center;
            align-items: center;
            width: 100%;
            height: 20%;
            background-color: #333;
            color: white;
        }
        .control-button {
            margin: 0 10px;
            padding: 10px 20px;
            background-color: #555;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            font-size: 16px;
            color: white;
        }
        .control-button:hover {
            background-color: #777;
        }
    </style>
</head>
<body>
       <!-- Container for video elements -->
    <div id="video-container">
        <div class="video-wrapper" id="localVideo-wrapper">
            <video id="localVideo" autoplay playsinline muted></video>
            <canvas id="outputCanvas"></canvas>
        </div>
             <!-- Wrapper for remote video -->
        <div class="video-wrapper" id="remoteVideo-wrapper">
            <video id="remoteVideo" autoplay playsinline></video>
        </div>
    </div>
    <div id="controls">
        <button class="control-button" id="muteButton">Mute</button>
        <button class="control-button" id="cameraButton">Turn Off Camera</button>
        <button class="control-button" id="downloadButton" onclick="downloadCSV()">Download CSV</button>
    </div>
    
    <script>
         // Get references to HTML elements
        const localVideo = document.getElementById('localVideo');
        const remoteVideo = document.getElementById('remoteVideo');
        const canvasElement = document.getElementById('outputCanvas');
        const canvasCtx = canvasElement.getContext('2d');
        const videoWrappers = document.querySelectorAll('.video-wrapper');

        let localStream;
        let handMovements = [];// Array to store hand movement data
        const loggingInterval = 5000;// Time in ms between logs, changed from 500000 for demo

            // Initialize MediaPipe Hands
        const hands = new Hands({
            locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`
        });

        // Configure hand detection options
        hands.setOptions({
            maxNumHands: 4,
            modelComplexity: 1,
            minDetectionConfidence: 0.5,
            minTrackingConfidence: 0.5
        });

          // Set the callback function for hand detection results
        hands.onResults(onResults);

               // Function to handle hand detection results
        function onResults(results) {
            canvasCtx.save();
            canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
            canvasCtx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);
            if (results.multiHandLandmarks) {
                for (const landmarks of results.multiHandLandmarks) {
                       // Draw hand connections
                    drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS, {color: 'lime', lineWidth: 4});
                        // Draw hand landmarks
                    drawLandmarks(canvasCtx, landmarks, {color: 'red', radius: 5});
                }
            }
            canvasCtx.restore();
            requestAnimationFrame(predictHands);
        }

          // Function to start video stream
        function startVideo(videoElement) {
            navigator.mediaDevices.getUserMedia({ video: true, audio: true })
                .then(stream => {
                    videoElement.srcObject = stream;
                    localStream = stream;
                    stream.getTracks().forEach(track => {
                        peerConnection.addTrack(track, stream);
                    });
                    videoElement.play();
                    videoElement.onloadedmetadata = () => {
                        canvasElement.width = videoElement.videoWidth;
                        canvasElement.height = videoElement.videoHeight;
                        predictHands();
                        startCall();  // Call startCall here after stream is ready
                    };
                })
                .catch(error => {
                    console.error('Error opening video camera:', error);
                });
        }
            // Function to start hand detection
        function predictHands() {
            hands.send({image: localVideo});
        }

           // Start the local video stream
        startVideo(localVideo);

         // Create RTCPeerConnection for WebRTC
        const peerConnection = new RTCPeerConnection({
            iceServers: [{ urls: 'stun:stun.l.google.com:19302' }],
            iceTransportPolicy: 'all',
            iceCandidatePoolSize: 10,
            bundlePolicy: 'max-bundle',
            rtcpMuxPolicy: 'require',
            sdpSemantics: 'unified-plan'
        });

         // Listen for tracks added to the peer connection
        peerConnection.ontrack = (event) => {
            if (!remoteVideo.srcObject) {
                remoteVideo.srcObject = new MediaStream();
            }
            remoteVideo.srcObject.addTrack(event.track);
        };

             // Handle ICE candidates
        peerConnection.onicecandidate = event => {
            if (event.candidate) {
                socket.emit('ice-candidate', { candidate: event.candidate });
            }
        };

        peerConnection.oniceconnectionstatechange = () => {
            console.log('ICE connection state:', peerConnection.iceConnectionState);
        };

        peerConnection.onsignalingstatechange = () => {
            console.log('Signaling state:', peerConnection.signalingState);
        };

              // Implement adaptive bitrate
        peerConnection.addEventListener('iceconnectionstatechange', () => {
            if (peerConnection.iceConnectionState === 'connected') {
                peerConnection.getReceivers().forEach(receiver => {
                    if (receiver.track.kind === 'video') {
                        receiver.setParameters({
                            degradationPreference: 'maintain-framerate'
                        });
                    }
                });
            }
        });

        // Initialize Socket.IO connection
        const socket = io({transports: ['websocket']}); // Automatically connect to the host that serves the page
        socket.on('connect', () => {
            console.log('Socket.IO connected');
        });
        socket.on('disconnect', () => {
            console.log('Socket.IO disconnected');
        });

        // Handle incoming offer
        socket.on('offer', data => {
            peerConnection.setRemoteDescription(new RTCSessionDescription(data.offer))
                .then(() => peerConnection.createAnswer())
                .then(answer => {
                    peerConnection.setLocalDescription(answer);
                    socket.emit('answer', { answer });
                })
                .catch(e => console.error('Error handling offer:', e));
        });

         // Handle incoming answer
        socket.on('answer', data => {
            peerConnection.setRemoteDescription(new RTCSessionDescription(data.answer))
                .catch(e => console.error('Error setting remote description with answer', e));
        });

        // Handle incoming ICE candidates
        socket.on('ice-candidate', data => {
            if (data.candidate) {
                let candidate = new RTCIceCandidate(data.candidate);
                peerConnection.addIceCandidate(candidate)
                    .catch(e => console.error('Error adding received ICE candidate', e));
            }
        });

        // Function to initiate a call
        function startCall() {
            console.log('Starting call...');
            if (localStream) {
                peerConnection.createOffer()
                    .then(offer => {
                        console.log('Offer created');
                        return peerConnection.setLocalDescription(offer);
                    })
                    .then(() => {
                        console.log('Local description set, sending offer');
                        socket.emit('offer', { offer: peerConnection.localDescription });
                    })
                    .catch(e => console.error('Error in startCall:', e));
            } else {
                console.log('Waiting for local stream');
            }
        }

         // Function to download hand movement data as CSV
        function downloadCSV() {
            const rows = [
                ["time", "handIndex", "landmarkIndex", "x", "y", "z"]
            ];
            handMovements.forEach(move => {
                move.landmarks.forEach((landmark, index) => {
                    rows.push([move.time, move.handIndex, index, landmark.x, landmark.y, landmark.z]);
                });
            });

            let csvContent = "data:text/csv;charset=utf-8,";
            rows.forEach(rowArray => {
                let row = rowArray.join(",");
                csvContent += row + "\r\n";
            });

            var encodedUri = encodeURI(csvContent);
            var link = document.createElement("a");
            link.setAttribute("href", encodedUri);
            link.setAttribute("download", "hand_movements.csv");
            document.body.appendChild(link);
            link.click();
            document.body.removeChild(link);
        }

        // Function to log hand movements
        function logHandMovements(results) {
            if (results.multiHandLandmarks) {
                results.multiHandLandmarks.forEach((landmarks, index) => {
                    handMovements.push({
                        time: Date.now(),
                        handIndex: index,
                        landmarks: landmarks.map(l => ({x: l.x * canvasElement.width, y: l.y * canvasElement.height, z: l.z}))
                    });
                });
            }
        }

        // Set up hand detection results handler
        hands.onResults((results) => {
            onResults(results);
            logHandMovements(results);
        });

       // Mute/Unmute functionality
const muteButton = document.getElementById('muteButton');
muteButton.addEventListener('click', () => {
    const audioTracks = localStream.getAudioTracks();
    if (audioTracks.length > 0) {
        const isCurrentlyMuted = !audioTracks[0].enabled;
        audioTracks.forEach(track => track.enabled = !isCurrentlyMuted);
        muteButton.textContent = isCurrentlyMuted ? 'Mute' : 'Unmute';
    }
});


        // Turn On/Off Camera functionality
        const cameraButton = document.getElementById('cameraButton');
        cameraButton.addEventListener('click', () => {
            const videoTracks = localStream.getVideoTracks();
            if (videoTracks.length > 0) {
                const isCameraOff = !videoTracks[0].enabled;
                videoTracks.forEach(track => track.enabled = isCameraOff);
                cameraButton.textContent = isCameraOff ? 'Turn On Camera' : 'Turn Off Camera';
            }
        });

        // Active speaker highlight functionality
        let activeVideo = localVideo; // Assume local video is initially active
        videoWrappers.forEach(wrapper => {
            wrapper.addEventListener('click', () => {
                // Remove active class from all wrappers
                videoWrappers.forEach(w => w.classList.remove('active'));
                // Add active class to clicked wrapper
                wrapper.classList.add('active');
                // Update active video based on wrapper clicked
                activeVideo = wrapper.id === 'localVideo-wrapper' ? localVideo : remoteVideo;
            });
        });
    </script>
</body>
</html>
